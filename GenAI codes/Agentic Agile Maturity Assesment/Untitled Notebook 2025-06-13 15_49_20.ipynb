{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "019c2d18-24ab-4f97-9864-35341ae21b60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install numpy==1.24.4 --no-cache-dir\n",
    "%pip install --force-reinstall --no-deps langchain langchain-community langchain-openai\n",
    "%pip install databricks-sdk\n",
    "%pip install -U duckduckgo-search\n",
    "%pip install langchain-core\n",
    "%pip install langchain-text-splitters\n",
    "%pip install aiohttp\n",
    "dbutils.library.restartPython()  # Restart the kernel after installing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02f79463-fe33-46e6-a910-8be8db9cc747",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "940a856b-85bc-499b-ab9b-5fb08a6d41bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 1: Install if needed\n",
    "# %pip install langchain databricks-genai matplotlib\n",
    "\n",
    "# Cell 2: Imports\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.prompts import PromptTemplate\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Databricks LLM Setup\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=\"databricks-llama-4-maverick\",\n",
    "    temperature=0.3,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# Dimensions of Agile Maturity\n",
    "dimensions = [\n",
    "    \"Sprint Planning\", \"Team Collaboration\", \"CI/CD & DevOps\",\n",
    "    \"Stakeholder Engagement\", \"Retrospectives\"\n",
    "]\n",
    "\n",
    "# Sample transcript\n",
    "transcript = \"\"\"\n",
    "[Consultant]: Are you conducting retrospectives regularly?\n",
    "[Dev1]: We do, but not after every sprint.\n",
    "[Consultant]: How often do you release code?\n",
    "[Dev2]: Usually every 2 weeks. Sometimes there's a delay.\n",
    "[Consultant]: Are all teams using Scrum uniformly?\n",
    "[Dev3]: Not really, each team follows their own style.\n",
    "\"\"\"\n",
    "\n",
    "# Cell 3: Tool Template\n",
    "template = PromptTemplate.from_template(\"\"\"\n",
    "You're an Agile coach. Read the transcript below and evaluate the team's Agile maturity for the dimension: \"{dimension}\".\n",
    "\n",
    "Transcript:\n",
    "{transcript}\n",
    "\n",
    "Give output:\n",
    "Score (1 to 5):\n",
    "Justification:\n",
    "\"\"\")\n",
    "\n",
    "# Tool function\n",
    "def assess_agile_dimension(input: str) -> str:\n",
    "    dim, txt = input.split(\"||\")\n",
    "    prompt = template.format(dimension=dim.strip(), transcript=txt.strip())\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "# Tools list for each dimension\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=dim,\n",
    "        func=lambda input, dim=dim: assess_agile_dimension(f\"{dim}||{input}\"),\n",
    "        description=f\"Assess Agile Maturity for {dim}\"\n",
    "    )\n",
    "    for dim in dimensions\n",
    "]\n",
    "\n",
    "# Cell 4: Agent Setup\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Cell 5: Run agent for each dimension\n",
    "results = {}\n",
    "for dim in dimensions:\n",
    "    print(f\"üîç Assessing: {dim}\")\n",
    "    result = agent.run(f\"Evaluate {dim} based on this transcript: {transcript}\")\n",
    "    results[dim] = result\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04f7082a-062d-48bb-800c-6a17f94e757f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ CONFIG\n",
    "# -------------------------------------\n",
    "TRANSCRIPT_FILE = \"/Workspace/Users/rajesh.ghosh@xebia.com/GenAi/GenAI codes/Agentic Agile Maturity Assesment/Maturity Assessment/transcripts.txt\"\n",
    "OUTPUT_EXCEL_FILE = \"/Workspace/Users/rajesh.ghosh@xebia.com/GenAi/GenAI codes/Agentic Agile Maturity Assesment/Maturity Assessment/agile_maturity_assessment.xlsx\"\n",
    "OUTPUT_JSON_FILE = \"/Workspace/Users/rajesh.ghosh@xebia.com/GenAi/GenAI codes/Agentic Agile Maturity Assesment/Maturity Assessment/agile_maturity_assessment.json\"\n",
    "DATABRICKS_LLM_ENDPOINT = \"databricks-llama-4-maverick\"\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ LLM Setup\n",
    "# -------------------------------------\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=DATABRICKS_LLM_ENDPOINT,\n",
    "    temperature=0.3,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Agile Maturity Dimensions\n",
    "# -------------------------------------\n",
    "dimensions = [\n",
    "    \"Sprint Planning\",\n",
    "    \"Team Collaboration\",\n",
    "    \"CI/CD & DevOps\",\n",
    "    \"Stakeholder Engagement\",\n",
    "    \"Retrospectives\"\n",
    "]\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Prompt Template\n",
    "# -------------------------------------\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an Agile consultant.\n",
    "\n",
    "Your task: evaluate the Agile maturity for the following dimension: \"{dimension}\" based strictly on this transcript:\n",
    "{transcript}\n",
    "\n",
    "‚ö†Ô∏è Follow this exact format:\n",
    "Score: <number from 1 to 5>\n",
    "Justification: <one short paragraph>\n",
    "\n",
    "Do not return Thought, Action, Observation, or Markdown.\n",
    "Do not explain your steps ‚Äî only return the final score and justification in plain text.\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Read Transcript File\n",
    "# -------------------------------------\n",
    "def read_transcript(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "transcript = read_transcript(TRANSCRIPT_FILE)\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Tool Function per Dimension\n",
    "# -------------------------------------\n",
    "def assess_agile_dimension(input: str) -> str:\n",
    "    dim, txt = input.split(\"||\")\n",
    "    prompt = prompt_template.format(dimension=dim.strip(), transcript=txt.strip())\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=dim,\n",
    "        func=lambda input, dim=dim: assess_agile_dimension(f\"{dim}||{input}\"),\n",
    "        description=f\"Assess Agile Maturity for {dim}\"\n",
    "    )\n",
    "    for dim in dimensions\n",
    "]\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Agent Execution\n",
    "# -------------------------------------\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "results = {}\n",
    "for dim in dimensions:\n",
    "    print(f\"\\nüîç Evaluating: {dim}\")\n",
    "    result = agent.run(f\"Evaluate {dim} based on this transcript: {transcript}\")\n",
    "    results[dim] = result\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Improved Final Output Parsing\n",
    "# -------------------------------------\n",
    "def extract_final_answer_v2(text):\n",
    "    \"\"\"\n",
    "    Extracts Score and Justification from LLM output text, handling various formatting issues.\n",
    "    \"\"\"\n",
    "    if not text or \"Agent stopped\" in text or \"error\" in text.lower():\n",
    "        return None, \"‚ùå Agent failed or timed out.\"\n",
    "\n",
    "    # Normalize text: remove extra whitespace and markdown symbols\n",
    "    text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub(r'[\\*\\#]+', '', text)  # Remove markdown symbols like ** or #\n",
    "\n",
    "    # 1. Extract Score: Flexible regex to match various score formats\n",
    "    score_match = re.search(r'score\\s*[:\\-]?\\s*(\\d)\\b', text, re.IGNORECASE)\n",
    "    score = int(score_match.group(1)) if score_match and score_match.group(1).isdigit() else None\n",
    "\n",
    "    # 2. Extract Justification\n",
    "    justification = \"Justification not found\"\n",
    "    justification_match = re.search(r'justification\\s*[:\\-]?\\s*(.+?)(?=\\s*(?:score|$))', text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if justification_match:\n",
    "        justification = justification_match.group(1).strip()\n",
    "    else:\n",
    "        # Fallback: Take all text after score or last non-empty paragraph\n",
    "        if score_match:\n",
    "            post_score = text[score_match.end():].strip()\n",
    "            if post_score:\n",
    "                justification = post_score\n",
    "        else:\n",
    "            # If no score, take the last non-empty paragraph\n",
    "            paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "            justification = paragraphs[-1] if paragraphs else justification\n",
    "\n",
    "    # Clean justification: remove any lingering markdown or extra spaces\n",
    "    justification = re.sub(r'\\s+', ' ', justification.strip())\n",
    "\n",
    "    # Validate score\n",
    "    if score is None or not 1 <= score <= 5:\n",
    "        score = None\n",
    "        justification = f\"Invalid or missing score: {justification}\"\n",
    "\n",
    "    return score, justification\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Format & Save to JSON\n",
    "# -------------------------------------\n",
    "output_data = []\n",
    "for dim, result in results.items():\n",
    "    score, justification = extract_final_answer_v2(result)\n",
    "    output_data.append({\n",
    "        \"Dimension\": dim,\n",
    "        \"Score\": score,\n",
    "        \"Justification\": justification\n",
    "    })\n",
    "\n",
    "# Save JSON output to file\n",
    "with open(OUTPUT_JSON_FILE, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ JSON saved at: {OUTPUT_JSON_FILE}\")\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Optional: Save to Excel\n",
    "# -------------------------------------\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
    "print(f\"\\n‚úÖ Excel saved at: {OUTPUT_EXCEL_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-06-13 15_49_20",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
