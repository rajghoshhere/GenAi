{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e364d7f-cd3f-4014-8f95-31f2eb0e88d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%pip install numpy==1.24.4 --no-cache-dir\n",
    "%pip install --force-reinstall --no-deps langchain langchain-community langchain-openai\n",
    "%pip install databricks-sdk\n",
    "%pip install -U duckduckgo-search\n",
    "%pip install langchain-core\n",
    "%pip install langchain-text-splitters\n",
    "%pip install aiohttp\n",
    "dbutils.library.restartPython()  # Restart the kernel after installing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffa9f14c-3479-4c9b-9dbb-0c0fcee9f103",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -U langchain\n",
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efb64ce1-5bff-4426-bad4-11c151870834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install openpyxl\n",
    "dbutils.library.restartPython() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4020e15a-d747-4675-8738-b678528588a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ CONFIG\n",
    "# -------------------------------------\n",
    "TRANSCRIPT_FILE = \"/Workspace/Users/rajesh.ghosh@xebia.com/GenAi/GenAI codes/Agentic Agile Maturity Assesment/Maturity Assessment/transcripts.txt\"\n",
    "OUTPUT_EXCEL_FILE = \"/Workspace/Users/rajesh.ghosh@xebia.com/GenAi/GenAI codes/Agentic Agile Maturity Assesment/Maturity Assessment/agile_maturity_assessment.xlsx\"\n",
    "OUTPUT_JSON_FILE = \"/Workspace/Users/rajesh.ghosh@xebia.com/GenAi/GenAI codes/Agentic Agile Maturity Assesment/Maturity Assessment/agile_maturity_assessment.json\"\n",
    "DATABRICKS_LLM_ENDPOINT = \"databricks-llama-4-maverick\"\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ LLM Setup\n",
    "# -------------------------------------\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=DATABRICKS_LLM_ENDPOINT,\n",
    "    temperature=0.3,\n",
    "    max_tokens=1000\n",
    ")\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Agile Maturity Dimensions\n",
    "# -------------------------------------\n",
    "dimensions = [\n",
    "    \"Sprint Planning\",\n",
    "    \"Team Collaboration\",\n",
    "    \"CI/CD & DevOps\",\n",
    "    \"Stakeholder Engagement\",\n",
    "    \"Retrospectives\"\n",
    "]\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Prompt Template\n",
    "# -------------------------------------\n",
    "prompt_template = PromptTemplate.from_template(\"\"\"\n",
    "You are an Agile consultant.\n",
    "\n",
    "Your task: evaluate the Agile maturity for the following dimension: \"{dimension}\" based strictly on this transcript:\n",
    "{transcript}\n",
    "\n",
    "‚ö†Ô∏è Follow this exact format:\n",
    "Score: <number from 1 to 5>\n",
    "Justification: <one short paragraph>\n",
    "\n",
    "Do not return Thought, Action, Observation, or Markdown.\n",
    "Do not explain your steps ‚Äî only return the final score and justification in plain text.\n",
    "\"\"\")\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Read Transcript File\n",
    "# -------------------------------------\n",
    "def read_transcript(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        return file.read()\n",
    "\n",
    "transcript = read_transcript(TRANSCRIPT_FILE)\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Tool Function per Dimension\n",
    "# -------------------------------------\n",
    "def assess_agile_dimension(input: str) -> str:\n",
    "    dim, txt = input.split(\"||\")\n",
    "    prompt = prompt_template.format(dimension=dim.strip(), transcript=txt.strip())\n",
    "    return llm.invoke(prompt).content\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=dim,\n",
    "        func=lambda input, dim=dim: assess_agile_dimension(f\"{dim}||{input}\"),\n",
    "        description=f\"Assess Agile Maturity for {dim}\"\n",
    "    )\n",
    "    for dim in dimensions\n",
    "]\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Agent Execution\n",
    "# -------------------------------------\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "results = {}\n",
    "for dim in dimensions:\n",
    "    print(f\"\\nüîç Evaluating: {dim}\")\n",
    "    result = agent.run(f\"Evaluate {dim} based on this transcript: {transcript}\")\n",
    "    results[dim] = result\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Improved Final Output Parsing\n",
    "# -------------------------------------\n",
    "def extract_final_answer_v2(text):\n",
    "    \"\"\"\n",
    "    Extracts Score and Justification from LLM output text, handling various formatting issues and edge cases.\n",
    "    \"\"\"\n",
    "    if not text or \"Agent stopped\" in text or \"error\" in text.lower():\n",
    "        return None, \"‚ùå Agent failed or timed out.\"\n",
    "\n",
    "    # Normalize text: remove extra whitespace and markdown symbols\n",
    "    #text = re.sub(r'\\s+', ' ', text.strip())\n",
    "    text = re.sub(r'[\\*\\#]+', '', text)  # Remove markdown symbols like ** or #\n",
    "\n",
    "    # 1. Extract Score: Try multiple formats\n",
    "    score = None\n",
    "    # Primary format: Score: X or score X\n",
    "    score_match = re.search(r'score\\s*[:\\-]?\\s*(\\d(\\.\\d)?)\\b', text, re.IGNORECASE)\n",
    "    if score_match:\n",
    "        score = float(score_match.group(1))\n",
    "    \n",
    "    # Fallback 1: Search for score in text like \"is X\" or \"level of X\"\n",
    "    if not score_match:\n",
    "        score_match = re.search(r'\\b(is|level\\s+of|rated\\s+a)\\s*(\\d(\\.\\d)?)\\b', text, re.IGNORECASE)\n",
    "        if score_match:\n",
    "            score = float(score_match.group(2))\n",
    "    \n",
    "    # Fallback 2: Any standalone number between 1 and 5 before justification/reasoning\n",
    "    if not score_match:\n",
    "        score_match = re.search(r'\\b(\\d(\\.\\d)?)\\b(?=.*?(?:justification|reasoning))', text, re.IGNORECASE)\n",
    "        if score_match:\n",
    "            score = float(score_match.group(1))\n",
    "\n",
    "    # Validate score: Ensure it's between 1 and 5\n",
    "    if score is not None and not 1 <= score <= 5:\n",
    "        score = None\n",
    "\n",
    "    # 2. Extract Justification\n",
    "    justification = \"Justification not found\"\n",
    "    justification_match = re.search(r'(?:justification|reasoning)\\s*[:\\-]?\\s*(.+?)(?=\\s*(?:score|$))', text, re.IGNORECASE | re.DOTALL)\n",
    "    \n",
    "    if justification_match:\n",
    "        justification = justification_match.group(1).strip()\n",
    "    else:\n",
    "        # Fallback: Take text after score or last non-empty paragraph\n",
    "        if score_match:\n",
    "            post_score = text[score_match.end():].strip()\n",
    "            if post_score:\n",
    "                justification = post_score\n",
    "        else:\n",
    "            # If no score, take the last non-empty paragraph\n",
    "            paragraphs = [p.strip() for p in text.split('\\n') if p.strip()]\n",
    "            justification = paragraphs[-1] if paragraphs else justification\n",
    "\n",
    "    # Clean justification: Remove embedded score and normalize\n",
    "    if score is not None:\n",
    "        justification = re.sub(rf'\\b{score}\\b', '', justification).strip()\n",
    "    justification = re.sub(r'\\s+', ' ', justification.strip())\n",
    "\n",
    "    # Special case: If no practices mentioned (e.g., CI/CD & DevOps), assign score 1\n",
    "    if score is None and \"does not contain relevant information\" in justification.lower():\n",
    "        score = 1.0\n",
    "        justification = justification.replace(\"Invalid or missing score: \", \"\")\n",
    "\n",
    "    # If no valid score was found, update justification to indicate the issue\n",
    "    if score is None and \"failed or timed out\" not in justification.lower():\n",
    "        justification = f\"Invalid or missing score: {justification}\"\n",
    "\n",
    "    # Ensure score is None if justification indicates failure\n",
    "    if \"failed or timed out\" in justification.lower():\n",
    "        score = None\n",
    "\n",
    "    return score, justification\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Format & Save to JSON\n",
    "# -------------------------------------\n",
    "output_data = []\n",
    "for dim, result in results.items():\n",
    "    score, justification = extract_final_answer_v2(result)\n",
    "    output_data.append({\n",
    "        \"Dimension\": dim,\n",
    "        \"Score\": score,\n",
    "        \"Justification\": justification\n",
    "    })\n",
    "\n",
    "# Save JSON output to file\n",
    "with open(OUTPUT_JSON_FILE, \"w\") as f:\n",
    "    json.dump(output_data, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ JSON saved at: {OUTPUT_JSON_FILE}\")\n",
    "\n",
    "# -------------------------------------\n",
    "# ‚úÖ Optional: Save to Excel\n",
    "# -------------------------------------\n",
    "df = pd.DataFrame(output_data)\n",
    "df.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
    "print(f\"\\n‚úÖ Excel saved at: {OUTPUT_EXCEL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "283a9b82-24c6-4b92-b3b7-298e30ecd7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------\n",
    "# ‚úÖ Convert JSON to Excel\n",
    "# -------------------------------------\n",
    "df_from_json = pd.read_json(OUTPUT_JSON_FILE)\n",
    "df_from_json.to_excel(OUTPUT_EXCEL_FILE, index=False)\n",
    "\n",
    "print(f\"‚úÖ Excel also saved at: {OUTPUT_EXCEL_FILE}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Agile Maturity Assessment",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
