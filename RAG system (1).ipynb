{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1a64dd1b-cde9-463e-8329-ecbeb4cb3472",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pypdf langchain faiss-cpu sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1eae7f-8f75-4e99-8776-7bb7fa07ca82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook: RAG System from Multiple PPTX Files with System Prompt\n",
    "\n",
    "## Step 1: Install Required Packages\n",
    "%pip install -U langchain-community sentence-transformers python-pptx faiss-cpu\n",
    "\n",
    "## Step 2: Extract Text from PPTX Files\n",
    "from pptx import Presentation\n",
    "import os\n",
    "\n",
    "def extract_text_from_pptx(file_path):\n",
    "    prs = Presentation(file_path)\n",
    "    text = \"\"\n",
    "    for slide in prs.slides:\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                text += shape.text + \"\\n\"\n",
    "    return text\n",
    "\n",
    "pptx_dir = \"your PPT stored location\"\n",
    "documents = []\n",
    "for filename in os.listdir(pptx_dir):\n",
    "    if filename.endswith(\".pptx\"):\n",
    "        full_path = os.path.join(pptx_dir, filename)\n",
    "        documents.append(extract_text_from_pptx(full_path))\n",
    "\n",
    "## Step 3: Chunk the Text\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = text_splitter.create_documents(documents)\n",
    "\n",
    "## Step 4: Embed and Save FAISS Index\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "faiss_store = FAISS.from_documents(docs, embedding_model)\n",
    "\n",
    "# Save FAISS index to disk\n",
    "index_dir = \"Your index storage\"\n",
    "faiss_store.save_local(index_dir)\n",
    "\n",
    "## Step 5: Load FAISS Index with safe deserialization\n",
    "faiss_store = FAISS.load_local(\n",
    "    folder_path=index_dir,\n",
    "    embeddings=embedding_model,\n",
    "    allow_dangerous_deserialization=True  # Use only if the index is trusted\n",
    ")\n",
    "\n",
    "## Step 6: Configure LLM with System Prompt\n",
    "from langchain.chat_models import ChatDatabricks\n",
    "from langchain.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Define your custom system prompt\n",
    "system_prompt = \"\"\"You are a helpful IT project Use case creator who extracts insights from corporate presentations.\n",
    "Be concise, accurate, and avoid repetition. Respond in bullet points where appropriate.Do not make things up, when you dont have context\"\"\"\n",
    "\n",
    "# Prompt template setup\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(system_prompt),\n",
    "    HumanMessagePromptTemplate.from_template(\"Context:\\n{context}\\n\\nQuestion: {question}\")\n",
    "])\n",
    "\n",
    "llm = ChatDatabricks(\n",
    "    endpoint=\"databricks-llama-4-maverick\",  \n",
    "    max_tokens=300\n",
    ")\n",
    "\n",
    "# Setup RetrievalQA chain using custom prompt\n",
    "rag_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=faiss_store.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": prompt}\n",
    ")\n",
    "\n",
    "## Step 7: Ask Multiple Questions\n",
    "questions = [\n",
    "    \"Which usecases are you covering?\",\n",
    "    \"What is outline of any use cases?\",\n",
    "    \"Where customer expectation was met?.\",\n",
    "    \"Who is Antony?\"\n",
    "    \n",
    "]\n",
    "\n",
    "for q in questions:\n",
    "    print(f\"Q: {q}\")\n",
    "    print(\"A:\", rag_chain.run(q))\n",
    "    print(\"------\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "RAG system (1)",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
